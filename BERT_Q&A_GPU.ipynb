{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markdown Guide",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7af41558603d4e4aa0ea5ec90fe790fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b51b9b2bd014d37a0c5cf19e694dac9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3b8fe3425b42427198f4a26091f5d19e",
              "IPY_MODEL_59104bf700744906b252e3ba60eb511c"
            ]
          }
        },
        "3b51b9b2bd014d37a0c5cf19e694dac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b8fe3425b42427198f4a26091f5d19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_066073cef07a408799dac5e8ade853b3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 443,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 443,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4abfa06d132c4276a06a9ee8bceb4891"
          }
        },
        "59104bf700744906b252e3ba60eb511c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d391283081004c91826fdad32b75325e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 443/443 [00:26&lt;00:00, 16.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6b1e224590c4b3ca0a7281fe4ef67bb"
          }
        },
        "066073cef07a408799dac5e8ade853b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4abfa06d132c4276a06a9ee8bceb4891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d391283081004c91826fdad32b75325e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6b1e224590c4b3ca0a7281fe4ef67bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fefcf2d0d696428f9f18696cec501b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3352325400664615a2faad0a2ef14b36",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_117f1ed265b8422ba99830132019c584",
              "IPY_MODEL_61d0389dc1a04ef4bf72139b581b24f6"
            ]
          }
        },
        "3352325400664615a2faad0a2ef14b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "117f1ed265b8422ba99830132019c584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f969ceea3234b2ca915d9f7f2d98777",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1340675298,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1340675298,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0f48aeca96d41d8a2ab6873e11dba8b"
          }
        },
        "61d0389dc1a04ef4bf72139b581b24f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cdb0709082f142a0a90fed862eaedb35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:18&lt;00:00, 74.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ba06a0969fa406c9bfc5a53152598c9"
          }
        },
        "6f969ceea3234b2ca915d9f7f2d98777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0f48aeca96d41d8a2ab6873e11dba8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdb0709082f142a0a90fed862eaedb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ba06a0969fa406c9bfc5a53152598c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d33e9e9948e74e4285a631da203dc3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_10db7c4a24e24c88adb2082c063e7fd4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe5abfdba9c642fb9009e4c9c7abb660",
              "IPY_MODEL_a94c060940834ac38cdf6ae812674caa"
            ]
          }
        },
        "10db7c4a24e24c88adb2082c063e7fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe5abfdba9c642fb9009e4c9c7abb660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_65c784d7f76749c09dc9c9a2a3bb95ad",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d0c825a15564a9c91d66c206d39ec14"
          }
        },
        "a94c060940834ac38cdf6ae812674caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_17d4cc7d949f46ee946856cc4948fa7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 2.04MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a37609ba0b6c46b4ba56cca58bf2eabd"
          }
        },
        "65c784d7f76749c09dc9c9a2a3bb95ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d0c825a15564a9c91d66c206d39ec14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "17d4cc7d949f46ee946856cc4948fa7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a37609ba0b6c46b4ba56cca58bf2eabd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maVdFSBvU1xz"
      },
      "source": [
        "##Install Libraries  - Tranformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaRvdoLEVPH4",
        "outputId": "48b388af-6764-49c2-f06a-b04cfeb93f3d"
      },
      "source": [
        "!pip install Transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 15.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 59.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from Transformers) (3.4.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from Transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from Transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from Transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from Transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->Transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->Transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->Transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=e73e6b9e81a3f294f058463aad8d916be5e1eb3ee12773d44a283a0f0ea9c5ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, Transformers\n",
            "Successfully installed Transformers-4.2.2 sacremoses-0.0.43 tokenizers-0.9.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZ-pveTV0UT"
      },
      "source": [
        "##Import libaries \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201,
          "referenced_widgets": [
            "7af41558603d4e4aa0ea5ec90fe790fc",
            "3b51b9b2bd014d37a0c5cf19e694dac9",
            "3b8fe3425b42427198f4a26091f5d19e",
            "59104bf700744906b252e3ba60eb511c",
            "066073cef07a408799dac5e8ade853b3",
            "4abfa06d132c4276a06a9ee8bceb4891",
            "d391283081004c91826fdad32b75325e",
            "a6b1e224590c4b3ca0a7281fe4ef67bb",
            "fefcf2d0d696428f9f18696cec501b0d",
            "3352325400664615a2faad0a2ef14b36",
            "117f1ed265b8422ba99830132019c584",
            "61d0389dc1a04ef4bf72139b581b24f6",
            "6f969ceea3234b2ca915d9f7f2d98777",
            "a0f48aeca96d41d8a2ab6873e11dba8b",
            "cdb0709082f142a0a90fed862eaedb35",
            "1ba06a0969fa406c9bfc5a53152598c9",
            "d33e9e9948e74e4285a631da203dc3b2",
            "10db7c4a24e24c88adb2082c063e7fd4",
            "fe5abfdba9c642fb9009e4c9c7abb660",
            "a94c060940834ac38cdf6ae812674caa",
            "65c784d7f76749c09dc9c9a2a3bb95ad",
            "8d0c825a15564a9c91d66c206d39ec14",
            "17d4cc7d949f46ee946856cc4948fa7e",
            "a37609ba0b6c46b4ba56cca58bf2eabd"
          ]
        },
        "id": "e24Ac1cpXacD",
        "outputId": "6857855d-3fbd-44e0-a694-0215cf1ca67b"
      },
      "source": [
        "import torch\r\n",
        "from transformers import BertForQuestionAnswering\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    dev = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    dev = torch.device(\"cpu\")\r\n",
        "\r\n",
        "\r\n",
        "#Model\r\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\r\n",
        "model = model.to(dev)\r\n",
        "#Tokenizer\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\r\n",
        "\r\n",
        "#Read test file from Github\r\n",
        "\r\n",
        "import requests\r\n",
        "url = \"https://raw.githubusercontent.com/newspap/MyRepo/main/BERT_for_dummies.txt\"\r\n",
        "resp = requests.get(url)\r\n",
        "input_ = resp.text.split(\"\\r\\n\") \r\n",
        "input_ = list(filter(lambda x: len(x) > 5, input_))\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7af41558603d4e4aa0ea5ec90fe790fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fefcf2d0d696428f9f18696cec501b0d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1340675298.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d33e9e9948e74e4285a631da203dc3b2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbD3KkFQZIOJ",
        "outputId": "9ae802ab-ed37-4253-f4a8-94d4f4016973"
      },
      "source": [
        "from random import sample \r\n",
        "input_ = resp.text.split(\"\\r\\n\") \r\n",
        "input_ = list(filter(lambda x: len(x) > 5, input_))\r\n",
        "print(sample(input_, 10))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Image for post', 'Sign up for The Daily Pick', 'Conclusion', 'Image for post', 'BERT is basically a trained Transformer Encoder stack, with twelve in the Base version, and twenty-four in the Large version, compared to 6 encoder layers in the original Transformer we described in the previous article.', 'A Medium publication sharing concepts, ideas, and codes.', 'When we use the trained model to predict the intents on the unseen test dataset, the confusion matrix clearly shows how the model overfits to the majority �flight� class.', 'Follow', 'In this article, we will demonstrate Transformer, especially how its attention mechanism helps in solving the intent classification task by learning contextual relationships. After demonstrating the limitation of a LSTM-based classifier, we introduce BERT: Pre-training of Deep Bidirectional Transformers, a novel Transformer-approach, pre-trained on large corpora and open-sourced. The last part of this article presents the Python code necessary for fine-tuning BERT for the task of Intent Classification and achieving state-of-art accuracy on unseen intent queries. We use the ATIS (Airline Travel Information System) dataset, a standard benchmark dataset widely used for recognizing the intent behind a customer query.', 'towardsdatascience.com']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GWU2AibZ-Sa"
      },
      "source": [
        "##Continue ask questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxhTyK7KaDjT",
        "outputId": "987a0fe6-39bd-4243-8d24-9bd3bfd0d3e1"
      },
      "source": [
        "class bcolors:\r\n",
        "    HEADER = '\\033[95m'\r\n",
        "    OKBLUE = '\\033[94m'\r\n",
        "    OKCYAN = '\\033[96m'\r\n",
        "    OKGREEN = '\\033[92m'\r\n",
        "    WARNING = '\\033[93m'\r\n",
        "    FAIL = '\\033[91m'\r\n",
        "    ENDC = '\\033[0m'\r\n",
        "    BOLD = '\\033[1m'\r\n",
        "    UNDERLINE = '\\033[4m'\r\n",
        "\r\n",
        "def group_words(s, n):\r\n",
        "    words = s.split()\r\n",
        "    for i in range(0, len(words), n):\r\n",
        "        yield ' '.join(words[i:i+n])\r\n",
        "\r\n",
        "def get_loss(ans):\r\n",
        "    return ans.get('loss')\r\n",
        "\r\n",
        "while True:\r\n",
        "    # if doc changed, reload it\r\n",
        "\r\n",
        "    answers=[]\r\n",
        "    question = input(bcolors.OKBLUE + \"Question: \" + bcolors.ENDC)\r\n",
        "\r\n",
        "    if question == '' :\r\n",
        "     \r\n",
        "        break\r\n",
        "    else:\r\n",
        "        for longParagraph in input_:\r\n",
        "            for paragraph in list(group_words(longParagraph, 180)):\r\n",
        "                #encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)\r\n",
        "\r\n",
        "                #inputs = encoding['input_ids']  #Token embeddings\r\n",
        "                #sentence_embedding = encoding['token_type_ids']  #Segment embeddings\r\n",
        "              \r\n",
        "                inputs = tokenizer(question, paragraph, return_tensors='pt')\r\n",
        "                tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0]) #input tokens\r\n",
        "                start_positions = torch.tensor([1])\r\n",
        "                end_positions = torch.tensor([3])\r\n",
        "\r\n",
        "                #outputs = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\r\n",
        "                with torch.no_grad():\r\n",
        "                    inputs = inputs.to(dev)\r\n",
        "                    start_positions = start_positions.to(dev)\r\n",
        "                    end_positions = end_positions.to(dev)\r\n",
        "                    outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\r\n",
        "                loss = outputs.loss\r\n",
        "                #start_scores, end_scores\r\n",
        "                start_index = torch.argmax(outputs.start_logits)\r\n",
        "\r\n",
        "                end_index = torch.argmax(outputs.end_logits)\r\n",
        "\r\n",
        "                answer = ' '.join(tokens[start_index:end_index+1])\r\n",
        "\r\n",
        "                corrected_answer = ''\r\n",
        "\r\n",
        "                for word in answer.split():\r\n",
        "                    \r\n",
        "                    #If it's a subword token\r\n",
        "                    if word[0:2] == '##':\r\n",
        "                        corrected_answer += word[2:]\r\n",
        "                    else:\r\n",
        "                        corrected_answer += ' ' + word\r\n",
        "                #if (corrected_answer.startswith(\" [\")):\r\n",
        "                #    print(corrected_answer)\r\n",
        "                lossVal = loss.item()\r\n",
        "                \r\n",
        "                if (len(corrected_answer) > 0):\r\n",
        "                    print(bcolors.FAIL + str(lossVal) + \": \" + bcolors.ENDC + bcolors.WARNING + corrected_answer + bcolors.ENDC) \r\n",
        "                    #print(paragraph)\r\n",
        "                answers.append({\"loss\":lossVal, \"answer\":corrected_answer, \"paragraph\": paragraph})\r\n",
        "    answers.sort(key=get_loss)\r\n",
        "    print(\"-------------------------- Question -----------------------------------------------------\")\r\n",
        "    print(\":: \" + question)\r\n",
        "    print(\"------------------------- Answer  ------------------------------------------------------\")\r\n",
        "    \r\n",
        "    for ans in answers[-15:]:\r\n",
        "        coloredAns = (bcolors.OKGREEN if ans['loss'] > 10 else bcolors.FAIL) + str(ans['loss']) + \": \" + bcolors.ENDC + bcolors.WARNING + ans['answer'] + bcolors.ENDC\r\n",
        "\r\n",
        "        print(coloredAns)\r\n",
        "        startIdx = ans['paragraph'].lower().find(ans['answer'])\r\n",
        "\r\n",
        "        ansLength = len(ans['answer'])\r\n",
        "\r\n",
        "        if (startIdx != -1):\r\n",
        "            hightlightedPara = ans['paragraph'][0:startIdx] + bcolors.WARNING + \\\r\n",
        "                    ans['paragraph'][startIdx:startIdx + ansLength] + bcolors.ENDC\r\n",
        "            if (startIdx + ansLength < len(ans['paragraph'])):\r\n",
        "                hightlightedPara += ans['paragraph'][startIdx + ansLength]\r\n",
        "        else :\r\n",
        "            hightlightedPara = ans['paragraph']\r\n",
        "        print(hightlightedPara)\r\n",
        "\r\n",
        "    print(\"############################### For Question ###############################\")\r\n",
        "    print(bcolors.HEADER +  question  + bcolors.ENDC)\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mQuestion: \u001b[0mwhat is the topic?\n",
            "\u001b[91m11.030708312988281: \u001b[0m\u001b[93m motivation\u001b[0m\n",
            "\u001b[91m8.805683135986328: \u001b[0m\u001b[93m classify queries into specific intents in order to generate the most coherent response . intent classification\u001b[0m\n",
            "\u001b[91m9.975690841674805: \u001b[0m\u001b[93m question answering\u001b[0m\n",
            "\u001b[91m7.8452959060668945: \u001b[0m\u001b[93m understanding the intuition\u001b[0m\n",
            "\u001b[91m10.174060821533203: \u001b[0m\u001b[93m intent labeling\u001b[0m\n",
            "\u001b[91m9.011407852172852: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "\u001b[91m11.935047149658203: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "\u001b[91m8.513883590698242: \u001b[0m\u001b[93m training dataset , we have 26 distinct intents , whose distribution is shown below . the dataset is highly unbalanced , with most queries labeled as flight\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m9.21318531036377: \u001b[0m\u001b[93m multi - class classifier\u001b[0m\n",
            "\u001b[91m9.421148300170898: \u001b[0m\u001b[93m classification\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m8.305319786071777: \u001b[0m\u001b[93m class flight\u001b[0m\n",
            "\u001b[91m9.858010292053223: \u001b[0m\u001b[93m confusion matrix clearly shows how the model overfits to the majority flight class\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m12.315897941589355: \u001b[0m\u001b[93m data augmentation\u001b[0m\n",
            "\u001b[91m11.203878402709961: \u001b[0m\u001b[93m dealing with an imbalanced dataset is a common challenge when solving a classification task\u001b[0m\n",
            "\u001b[91m9.621801376342773: \u001b[0m\u001b[93m natural language understanding\u001b[0m\n",
            "\u001b[91m10.757534980773926: \u001b[0m\u001b[93m binary classifier\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m8.266752243041992: \u001b[0m\u001b[93m loss function to binary crossentropy\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m7.557071208953857: \u001b[0m\u001b[93m flight\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m12.241024017333984: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "\u001b[91m11.218923568725586: \u001b[0m\u001b[93m poor classification result we witnessed with sequence - to - sequence models on the intent classification task\u001b[0m\n",
            "\u001b[91m6.840422630310059: \u001b[0m\u001b[93m transformer encoder stack\u001b[0m\n",
            "\u001b[91m10.366066932678223: \u001b[0m\u001b[93m lost in translation\u001b[0m\n",
            "\u001b[91m10.484125137329102: \u001b[0m\u001b[93m the mystery of transformer model\u001b[0m\n",
            "\u001b[91m7.080002307891846: \u001b[0m\u001b[93m wikipedia and book corpus , a dataset containing + 10 , 000 books of different genres\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m10.45888900756836: \u001b[0m\u001b[93m natural language processing\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m11.768914222717285: \u001b[0m\u001b[93m intent classification problem\u001b[0m\n",
            "\u001b[91m4.958515167236328: \u001b[0m\u001b[93m why do we need bert\u001b[0m\n",
            "\u001b[91m8.704459190368652: \u001b[0m\u001b[93m proper language representation is key for general - purpose language understanding by machines\u001b[0m\n",
            "\u001b[91m11.444103240966797: \u001b[0m\u001b[93m classification\u001b[0m\n",
            "\u001b[91m9.83618450164795: \u001b[0m\u001b[93m preparing bert environment\u001b[0m\n",
            "\u001b[91m5.247387886047363: \u001b[0m\u001b[93m jupyter notebook\u001b[0m\n",
            "\u001b[91m6.921062469482422: \u001b[0m\u001b[93m development environment\u001b[0m\n",
            "\u001b[91m8.663887023925781: \u001b[0m\u001b[93m natural language processing\u001b[0m\n",
            "\u001b[91m6.376486778259277: \u001b[0m\u001b[93m beginning ( [CLS] ) and separation / end of sentences\u001b[0m\n",
            "\u001b[91m5.580338478088379: \u001b[0m\u001b[93m denver\u001b[0m\n",
            "\u001b[91m5.589354515075684: \u001b[0m\u001b[93m [CLS]\u001b[0m\n",
            "\u001b[91m7.646951198577881: \u001b[0m\u001b[93m tokenized sentence\u001b[0m\n",
            "\u001b[91m6.957332611083984: \u001b[0m\u001b[93m data\u001b[0m\n",
            "\u001b[91m9.192913055419922: \u001b[0m\u001b[93m classification\u001b[0m\n",
            "\u001b[91m7.864016532897949: \u001b[0m\u001b[93m our specific task\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m12.965097427368164: \u001b[0m\u001b[93m natural language understanding\u001b[0m\n",
            "\u001b[91m8.017723083496094: \u001b[0m\u001b[93m training loss\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m8.430355072021484: \u001b[0m\u001b[93m the moment of truth\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m9.659547805786133: \u001b[0m\u001b[93m intent classification task\u001b[0m\n",
            "\u001b[91m7.455506324768066: \u001b[0m\u001b[93m conclusion\u001b[0m\n",
            "\u001b[91m11.019821166992188: \u001b[0m\u001b[93m natural language understanding tasks\u001b[0m\n",
            "\u001b[91m9.568923950195312: \u001b[0m\u001b[93m question answering\u001b[0m\n",
            "\u001b[91m9.975690841674805: \u001b[0m\u001b[93m question answering\u001b[0m\n",
            "\u001b[91m7.8452959060668945: \u001b[0m\u001b[93m understanding the intuition\u001b[0m\n",
            "\u001b[91m11.119365692138672: \u001b[0m\u001b[93m natural language understanding\u001b[0m\n",
            "\u001b[91m4.157814979553223: \u001b[0m\u001b[93m reading more from me .\u001b[0m\n",
            "\u001b[91m11.313129425048828: \u001b[0m\u001b[93m representing text in natural language processing\u001b[0m\n",
            "\u001b[91m10.173246383666992: \u001b[0m\u001b[93m understanding the written words\u001b[0m\n",
            "\u001b[91m10.986930847167969: \u001b[0m\u001b[93m generative adversarial network\u001b[0m\n",
            "\u001b[91m9.697400093078613: \u001b[0m\u001b[93m bulletproof python code\u001b[0m\n",
            "\u001b[91m11.561442375183105: \u001b[0m\u001b[93m tensorflow\u001b[0m\n",
            "\u001b[91m8.15867805480957: \u001b[0m\u001b[93m linear regression and gradient descent\u001b[0m\n",
            "\u001b[91m11.472990036010742: \u001b[0m\u001b[93m uncertainty in deep learning\u001b[0m\n",
            "\u001b[91m10.088083267211914: \u001b[0m\u001b[93m bayesian estimation of epistemic and aleatoric uncertainty\u001b[0m\n",
            "\u001b[91m8.755069732666016: \u001b[0m\u001b[93m curious minds become ai practitioner\u001b[0m\n",
            "\u001b[91m6.959158420562744: \u001b[0m\u001b[93m follow\u001b[0m\n",
            "\u001b[91m7.047304153442383: \u001b[0m\u001b[93m daily pick\u001b[0m\n",
            "\u001b[91m11.279218673706055: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m10.338933944702148: \u001b[0m\u001b[93m make learning your daily ritual\u001b[0m\n",
            "\u001b[91m5.867046356201172: \u001b[0m\u001b[93m your email\u001b[0m\n",
            "\u001b[91m4.390910625457764: \u001b[0m\u001b[93m get this newsletter\u001b[0m\n",
            "\u001b[91m9.828392028808594: \u001b[0m\u001b[93m privacy policy for more information about our privacy practices\u001b[0m\n",
            "\u001b[91m11.952064514160156: \u001b[0m\u001b[93m machine learning\u001b[0m\n",
            "\u001b[91m10.946022033691406: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m11.133224487304688: \u001b[0m\u001b[93m artificial intelligence\u001b[0m\n",
            "\u001b[91m10.719664573669434: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m10.847664833068848: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m6.959158420562744: \u001b[0m\u001b[93m follow\u001b[0m\n",
            "\u001b[91m9.726530075073242: \u001b[0m\u001b[93m sharing concepts , ideas , and codes\u001b[0m\n",
            "\u001b[91m8.20170783996582: \u001b[0m\u001b[93m monarch\u001b[0m\n",
            "\u001b[91m11.290607452392578: \u001b[0m\u001b[93m diversity\u001b[0m\n",
            "\u001b[91m9.84092903137207: \u001b[0m\u001b[93m diversity sampling\u001b[0m\n",
            "\u001b[91m12.206011772155762: \u001b[0m\u001b[93m diversity sampling\u001b[0m\n",
            "\u001b[91m9.656961441040039: \u001b[0m\u001b[93m model - based outliers\u001b[0m\n",
            "\u001b[91m9.245715141296387: \u001b[0m\u001b[93m cluster - based sampling\u001b[0m\n",
            "\u001b[91m10.633625030517578: \u001b[0m\u001b[93m the basics of indexing and slicing python lists\u001b[0m\n",
            "\u001b[91m8.502205848693848: \u001b[0m\u001b[93m a guide for beginners\u001b[0m\n",
            "\u001b[91m7.930637359619141: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m10.028169631958008: \u001b[0m\u001b[93m accessing the items in a list\u001b[0m\n",
            "\u001b[91m9.806915283203125: \u001b[0m\u001b[93m definitions and stage - setting\u001b[0m\n",
            "\u001b[91m10.616620063781738: \u001b[0m\u001b[93m indexing\u001b[0m\n",
            "-------------------------- Question -----------------------------------------------------\n",
            ":: what is the topic?\n",
            "------------------------- Answer  ------------------------------------------------------\n",
            "\u001b[92m11.203878402709961: \u001b[0m\u001b[93m dealing with an imbalanced dataset is a common challenge when solving a classification task\u001b[0m\n",
            "Dealing with an imbalanced dataset is a common challenge when solving a classification task. Data augmentation is one thing that comes to mind as a good workaround. Here, it is not rare to encounter the SMOTE algorithm, as a popular choice for augmenting the dataset without biasing predictions. SMOTE uses a k-Nearest Neighbors classifier to create synthetic datapoints as a multi-dimensional interpolation of closely related groups of true data points. Unfortunately, we have 25 minority classes in the ATIS training dataset, leaving us with a single overly representative class. SMOTE fails to work as it cannot find enough neighbors (minimum is 2). Oversampling with replacement is an alternative to SMOTE, which also does not improve the model�s predictive performance either.\n",
            "\u001b[92m11.218923568725586: \u001b[0m\u001b[93m poor classification result we witnessed with sequence - to - sequence models on the intent classification task\u001b[0m\n",
            "The motivation why we are now looking at Transformer is the poor classification result we witnessed with sequence-to-sequence models on the Intent Classification task when the dataset is imbalanced. In this section, we introduce a variant of Transformer and implement it for solving our classification problem. We will look especially at the late 2018 published Bidirectional Encoder Representations from Transformers (BERT).\n",
            "\u001b[92m11.279218673706055: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "By Towards\u001b[93m Data Science\u001b[0m\n",
            "\u001b[92m11.290607452392578: \u001b[0m\u001b[93m diversity\u001b[0m\n",
            "Diversity Sampling Cheatsheet\n",
            "\u001b[92m11.313129425048828: \u001b[0m\u001b[93m representing text in natural language processing\u001b[0m\n",
            "Representing text in natural language processing\n",
            "\u001b[92m11.444103240966797: \u001b[0m\u001b[93m classification\u001b[0m\n",
            "We will use BERT to extract high-quality language features from the ATIS query text data, and fine-tune BERT on a specific task (classification) with own data to produce state of the art predictions.\n",
            "\u001b[92m11.472990036010742: \u001b[0m\u001b[93m uncertainty in deep learning\u001b[0m\n",
            "Uncertainty in Deep Learning. How To Measure?\n",
            "\u001b[92m11.561442375183105: \u001b[0m\u001b[93m tensorflow\u001b[0m\n",
            "The Ultimate Beginner Guide to\u001b[93m TensorFlow\u001b[0m\n",
            "\u001b[92m11.768914222717285: \u001b[0m\u001b[93m intent classification problem\u001b[0m\n",
            "BERT works similarly to the Transformer encoder stack, by taking a sequence of words as input which keep flowing up the stack from one encoder to the next, while new sequences are coming in. The final output for each sequence is a vector of 728 numbers in Base or 1024 in Large version. We will use such vectors for our\u001b[93m intent classification problem\u001b[0m.\n",
            "\u001b[92m11.935047149658203: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "Intent classification with LSTM\n",
            "\u001b[92m11.952064514160156: \u001b[0m\u001b[93m machine learning\u001b[0m\n",
            "Machine Learning\n",
            "\u001b[92m12.206011772155762: \u001b[0m\u001b[93m diversity sampling\u001b[0m\n",
            "The four types of\u001b[93m Diversity Sampling\u001b[0m \n",
            "\u001b[92m12.241024017333984: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "Intent Classification with BERT\n",
            "\u001b[92m12.315897941589355: \u001b[0m\u001b[93m data augmentation\u001b[0m\n",
            "Data augmentation\n",
            "\u001b[92m12.965097427368164: \u001b[0m\u001b[93m natural language understanding\u001b[0m\n",
            "BERT fine-tuned on the Intent Classification task for\u001b[93m Natural Language Understanding\u001b[0m\n",
            "############################### For Question ###############################\n",
            "\u001b[95mwhat is the topic?\u001b[0m\n",
            "\n",
            "\u001b[94mQuestion: \u001b[0mwho is the author?\n",
            "\u001b[91m5.804155349731445: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "\u001b[91m8.696260452270508: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.175783157348633: \u001b[0m\u001b[93m pytorch code for bert\u001b[0m\n",
            "\u001b[91m6.902547359466553: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m8.961222648620605: \u001b[0m\u001b[93m liu and lane\u001b[0m\n",
            "\u001b[91m4.540616989135742: \u001b[0m\u001b[93m python\u001b[0m\n",
            "\u001b[91m6.480648517608643: \u001b[0m\u001b[93m lstm\u001b[0m\n",
            "\u001b[91m5.554750919342041: \u001b[0m\u001b[93m python\u001b[0m\n",
            "\u001b[91m5.14381742477417: \u001b[0m\u001b[93m multi - class classifier\u001b[0m\n",
            "\u001b[91m5.498488426208496: \u001b[0m\u001b[93m adam optimizer\u001b[0m\n",
            "\u001b[91m5.329429626464844: \u001b[0m\u001b[93m confusion matrix\u001b[0m\n",
            "\u001b[91m6.259000301361084: \u001b[0m\u001b[93m data augmentation\u001b[0m\n",
            "\u001b[91m6.491793632507324: \u001b[0m\u001b[93m [SEP]\u001b[0m\n",
            "\u001b[91m4.95034122467041: \u001b[0m\u001b[93m snips\u001b[0m\n",
            "\u001b[91m8.72500991821289: \u001b[0m\u001b[93m binary classifier\u001b[0m\n",
            "\u001b[91m4.962514400482178: \u001b[0m\u001b[93m we will rather reduce the scope of the problem . we define a binary classification\u001b[0m\n",
            "\u001b[91m5.600213050842285: \u001b[0m\u001b[93m flight\u001b[0m\n",
            "\u001b[91m9.828645706176758: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.55385160446167: \u001b[0m\u001b[93m bidirectional encoder representations from transformers ( bert ) .\u001b[0m\n",
            "\u001b[91m8.068873405456543: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.878260612487793: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m9.986814498901367: \u001b[0m\u001b[93m transformer\u001b[0m\n",
            "\u001b[91m8.710365295410156: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.902547359466553: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m7.005354881286621: \u001b[0m\u001b[93m wikipedia\u001b[0m\n",
            "\u001b[91m6.600233554840088: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m10.933005332946777: \u001b[0m\u001b[93m jay . alammar\u001b[0m\n",
            "\u001b[91m7.90632438659668: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.091367721557617: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.824549674987793: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m7.12730598449707: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.752263069152832: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.507135391235352: \u001b[0m\u001b[93m jupyter\u001b[0m\n",
            "\u001b[91m6.573843955993652: \u001b[0m\u001b[93m google\u001b[0m\n",
            "\u001b[91m6.092587947845459: \u001b[0m\u001b[93m hugging face\u001b[0m\n",
            "\u001b[91m4.894924163818359: \u001b[0m\u001b[93m torch\u001b[0m\n",
            "\u001b[91m5.353289604187012: \u001b[0m\u001b[93m python\u001b[0m\n",
            "\u001b[91m8.320510864257812: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m4.975320816040039: \u001b[0m\u001b[93m [CLS] i want to fly from boston\u001b[0m\n",
            "\u001b[91m9.122138977050781: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m7.392752647399902: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.1645402908325195: \u001b[0m\u001b[93m bertforsequenceclassification\u001b[0m\n",
            "\u001b[91m5.299806594848633: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m5.5678510665893555: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m4.31298828125: \u001b[0m\u001b[93m variable train _ loss _ set\u001b[0m\n",
            "\u001b[91m6.389200687408447: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.514443397521973: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m3.970341205596924: \u001b[0m\u001b[93m the author ? [SEP]\u001b[0m\n",
            "\u001b[91m5.662041664123535: \u001b[0m\u001b[93m pytorch\u001b[0m\n",
            "\u001b[91m8.15328598022461: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m8.696260452270508: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "\u001b[91m6.175783157348633: \u001b[0m\u001b[93m pytorch code for bert\u001b[0m\n",
            "\u001b[91m6.902547359466553: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m4.633449554443359: \u001b[0m\u001b[93m me .\u001b[0m\n",
            "\u001b[91m5.066873073577881: \u001b[0m\u001b[93m word2vec\u001b[0m\n",
            "\u001b[91m6.902547359466553: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m5.905777931213379: \u001b[0m\u001b[93m generative adversarial network ( gan ) for dummies\u001b[0m\n",
            "\u001b[91m6.462606906890869: \u001b[0m\u001b[93m bulletproof python code\u001b[0m\n",
            "\u001b[91m6.902547359466553: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m6.171753883361816: \u001b[0m\u001b[93m the ultimate beginner guide to tensorflow\u001b[0m\n",
            "\u001b[91m7.090887069702148: \u001b[0m\u001b[93m tensorflow\u001b[0m\n",
            "\u001b[91m6.902547359466553: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m5.634532451629639: \u001b[0m\u001b[93m deep learning\u001b[0m\n",
            "\u001b[91m9.254261016845703: \u001b[0m\u001b[93m keras\u001b[0m\n",
            "\u001b[91m6.902547359466553: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m11.916309356689453: \u001b[0m\u001b[93m michel kana\u001b[0m\n",
            "\u001b[91m8.267436981201172: \u001b[0m\u001b[93m fellow top medium writer\u001b[0m\n",
            "\u001b[91m4.277021884918213: \u001b[0m\u001b[93m daily pick\u001b[0m\n",
            "\u001b[91m5.412972450256348: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m4.545186519622803: \u001b[0m\u001b[93m medium\u001b[0m\n",
            "\u001b[91m5.90938138961792: \u001b[0m\u001b[93m machine learning\u001b[0m\n",
            "\u001b[91m4.702157020568848: \u001b[0m\u001b[93m a medium publication sharing concepts , ideas , and codes .\u001b[0m\n",
            "\u001b[91m10.25578498840332: \u001b[0m\u001b[93m robert ( munro ) monarch\u001b[0m\n",
            "\u001b[91m8.317361831665039: \u001b[0m\u001b[93m cheatsheet\u001b[0m\n",
            "\u001b[91m3.722890853881836: \u001b[0m\u001b[93m diversity sampling\u001b[0m\n",
            "\u001b[91m4.57891845703125: \u001b[0m\u001b[93m cheatsheet\u001b[0m\n",
            "\u001b[91m4.129671096801758: \u001b[0m\u001b[93m model - based outliers : sampling for low activation in your logits and hidden layers to find items that are confusing to your model\u001b[0m\n",
            "\u001b[91m4.375771999359131: \u001b[0m\u001b[93m unsupervised machine learning\u001b[0m\n",
            "\u001b[91m9.020689964294434: \u001b[0m\u001b[93m joseph h\u001b[0m\n",
            "\u001b[91m7.244735240936279: \u001b[0m\u001b[93m python\u001b[0m\n",
            "\u001b[91m10.39118480682373: \u001b[0m\u001b[93m a beginner\u001b[0m\n",
            "\u001b[91m6.526176452636719: \u001b[0m\u001b[93m igor miske\u001b[0m\n",
            "\u001b[91m5.902226448059082: \u001b[0m\u001b[93m python\u001b[0m\n",
            "\u001b[91m7.1998701095581055: \u001b[0m\u001b[93m python\u001b[0m\n",
            "\u001b[91m3.9120137691497803: \u001b[0m\u001b[93m indexing means referring to an element of an iterable by its position within the iterable .\u001b[0m\n",
            "-------------------------- Question -----------------------------------------------------\n",
            ":: who is the author?\n",
            "------------------------- Answer  ------------------------------------------------------\n",
            "\u001b[91m8.320510864257812: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "BERT expects input data in a specific format, with special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). Furthermore, we need to tokenize our text into tokens that correspond to\u001b[93m BERT\u001b[0m�\n",
            "\u001b[91m8.696260452270508: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "BERT NLP � How To Build a Question Answering Bot\n",
            "\u001b[91m8.696260452270508: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "BERT NLP � How To Build a Question Answering Bot\n",
            "\u001b[91m8.710365295410156: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "Tackle the mystery of Transformer model used by GPT-2,\u001b[93m BERT\u001b[0m\n",
            "\u001b[91m8.72500991821289: \u001b[0m\u001b[93m binary classifier\u001b[0m\n",
            "Binary classifier\n",
            "\u001b[91m8.961222648620605: \u001b[0m\u001b[93m liu and lane\u001b[0m\n",
            "The examples above show how ambiguous intent labeling can be. Users might add misleading words, causing multiple intents to be present in the same query. Attention-based learning methods were proposed for intent classification (Liu and Lane, 2016; Goo et al., 2018). One type of network built with attention is called a Transformer. It applies attention mechanisms to gather information about the relevant context of a given word, and then encode that context in a rich vector that smartly represents the word.\n",
            "\u001b[91m9.020689964294434: \u001b[0m\u001b[93m joseph h\u001b[0m\n",
            "Joseph H\n",
            "\u001b[91m9.122138977050781: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "For each tokenized sentence,\u001b[93m BERT\u001b[0m \n",
            "\u001b[91m9.254261016845703: \u001b[0m\u001b[93m keras\u001b[0m\n",
            "A hands-on tutorial on Bayesian estimation of epistemic and aleatoric uncertainty with\u001b[93m Keras\u001b[0m.\n",
            "\u001b[91m9.828645706176758: \u001b[0m\u001b[93m bert\u001b[0m\n",
            "Intent Classification with\u001b[93m BERT\u001b[0m\n",
            "\u001b[91m9.986814498901367: \u001b[0m\u001b[93m transformer\u001b[0m\n",
            "Lost in Translation. Found by\u001b[93m Transformer\u001b[0m.\n",
            "\u001b[92m10.25578498840332: \u001b[0m\u001b[93m robert ( munro ) monarch\u001b[0m\n",
            "Robert (Munro) Monarch\n",
            "\u001b[92m10.39118480682373: \u001b[0m\u001b[93m a beginner\u001b[0m\n",
            "A guide for beginners, by\u001b[93m a beginner\u001b[0m\n",
            "\u001b[92m10.933005332946777: \u001b[0m\u001b[93m jay . alammar\u001b[0m\n",
            "(source: Jay. Alammar, 2018)\n",
            "\u001b[92m11.916309356689453: \u001b[0m\u001b[93m michel kana\u001b[0m\n",
            "Michel Kana, Ph.D\n",
            "############################### For Question ###############################\n",
            "\u001b[95mwho is the author?\u001b[0m\n",
            "\n",
            "\u001b[94mQuestion: \u001b[0mhow to fine tune bert?\n",
            "\u001b[91m7.27348518371582: \u001b[0m\u001b[93m motivation\u001b[0m\n",
            "\u001b[91m8.381495475769043: \u001b[0m\u001b[93m classify queries into specific intents\u001b[0m\n",
            "\u001b[91m7.294320106506348: \u001b[0m\u001b[93m how to build a question answering bot\u001b[0m\n",
            "\u001b[91m11.897622108459473: \u001b[0m\u001b[93m understanding the intuition with hands - on pytorch code\u001b[0m\n",
            "\u001b[91m6.980084419250488: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m8.310941696166992: \u001b[0m\u001b[93m attention - based learning methods were proposed for intent classification ( liu and lane , 2016 ; goo et al . , 2018 ) . one type of network built with attention is called a transformer\u001b[0m\n",
            "\u001b[91m13.941883087158203: \u001b[0m\u001b[93m python code\u001b[0m\n",
            "\u001b[91m4.894338607788086: \u001b[0m\u001b[93m intent classification with lstm\u001b[0m\n",
            "\u001b[91m7.07435417175293: \u001b[0m\u001b[93m python code for loading the atis dataset\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m7.339902400970459: \u001b[0m\u001b[93m multi - class classifier\u001b[0m\n",
            "\u001b[91m9.862503051757812: \u001b[0m\u001b[93m softmax activation\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m7.797909736633301: \u001b[0m\u001b[93m the model appears to predict the majority class flight at each step\u001b[0m\n",
            "\u001b[91m9.397510528564453: \u001b[0m\u001b[93m the confusion matrix\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m8.012792587280273: \u001b[0m\u001b[93m data augmentation\u001b[0m\n",
            "\u001b[91m7.136630058288574: \u001b[0m\u001b[93m oversampling with replacement\u001b[0m\n",
            "\u001b[91m5.416591644287109: \u001b[0m\u001b[93m snips dataset\u001b[0m\n",
            "\u001b[91m7.094987869262695: \u001b[0m\u001b[93m binary classifier\u001b[0m\n",
            "\u001b[91m8.381597518920898: \u001b[0m\u001b[93m we will rather reduce the scope of the problem\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m7.587302207946777: \u001b[0m\u001b[93m the lstm model is still not able to learn to predict the intent\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m8.497329711914062: \u001b[0m\u001b[93m we have all samples being predicted as other\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m6.232341766357422: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "\u001b[91m9.200294494628906: \u001b[0m\u001b[93m bidirectional encoder representations from transformers\u001b[0m\n",
            "\u001b[91m2.939706563949585: \u001b[0m\u001b[93m how to fine tune bert ? [SEP]\u001b[0m\n",
            "\u001b[91m7.262221813201904: \u001b[0m\u001b[93m trained transformer encoder stack\u001b[0m\n",
            "\u001b[91m4.2709269523620605: \u001b[0m\u001b[93m lost in translation\u001b[0m\n",
            "\u001b[91m4.407732963562012: \u001b[0m\u001b[93m tackle the mystery of transformer model used by gpt - 2\u001b[0m\n",
            "\u001b[91m6.980084419250488: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m9.304485321044922: \u001b[0m\u001b[93m bert was trained on wikipedia and book corpus\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m11.107457160949707: \u001b[0m\u001b[93m minimal task - specific fine - tuning\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m10.128400802612305: \u001b[0m\u001b[93m taking a sequence of words as input which keep flowing up the stack from one encoder to the next , while new sequences are coming in\u001b[0m\n",
            "\u001b[91m11.36329460144043: \u001b[0m\u001b[93m fully bidirectional\u001b[0m\n",
            "\u001b[91m11.89180850982666: \u001b[0m\u001b[93m classification ) with own data\u001b[0m\n",
            "\u001b[91m7.7937211990356445: \u001b[0m\u001b[93m preparing bert environment\u001b[0m\n",
            "\u001b[91m7.925261974334717: \u001b[0m\u001b[93m code for verifying your gpu availability\u001b[0m\n",
            "\u001b[91m10.687943458557129: \u001b[0m\u001b[93m hugging face provides pytorch - transformers repository with additional libraries for interfacing more pre - trained models for natural language processing\u001b[0m\n",
            "\u001b[91m9.605649948120117: \u001b[0m\u001b[93m we load data onto that device\u001b[0m\n",
            "\u001b[91m8.494364738464355: \u001b[0m\u001b[93m please run the code from our previous article to preprocess the dataset using the python function load _ atis ( )\u001b[0m\n",
            "\u001b[91m9.322662353515625: \u001b[0m\u001b[93m special tokens to mark the beginning ( [CLS] ) and separation / end of sentences ( [SEP] ) . furthermore , we need to tokenize our text into tokens that correspond to berts vocabulary\u001b[0m\n",
            "\u001b[91m4.7250471115112305: \u001b[0m\u001b[93m [CLS] i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\u001b[0m\n",
            "\u001b[91m4.261911392211914: \u001b[0m\u001b[93m fine tune bert ? [SEP]\u001b[0m\n",
            "\u001b[91m9.81580924987793: \u001b[0m\u001b[93m input ids , a sequence of integers identifying each input token to its index number in the bert tokenizer vocabulary\u001b[0m\n",
            "\u001b[91m12.107767105102539: \u001b[0m\u001b[93m pre - training process\u001b[0m\n",
            "\u001b[91m12.368476867675781: \u001b[0m\u001b[93m using our data\u001b[0m\n",
            "\u001b[91m12.973657608032227: \u001b[0m\u001b[93m bertforsequenceclassification\u001b[0m\n",
            "\u001b[91m9.955744743347168: \u001b[0m\u001b[93m as we feed input data\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m9.574128150939941: \u001b[0m\u001b[93m the intent classification task for natural language understanding\u001b[0m\n",
            "\u001b[91m4.898526191711426: \u001b[0m\u001b[93m the training loss plot from the variable train _ loss _ set\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m11.362199783325195: \u001b[0m\u001b[93m pre - trained bert model it is possible to quickly and effectively create a high - quality model with minimal effort and training time using the pytorch interface\u001b[0m\n",
            "\u001b[91m10.690736770629883: \u001b[0m\u001b[93m on your own dataset\u001b[0m\n",
            "\u001b[91m12.024656295776367: \u001b[0m\u001b[93m on the squad dataset\u001b[0m\n",
            "\u001b[91m7.294320106506348: \u001b[0m\u001b[93m how to build a question answering bot\u001b[0m\n",
            "\u001b[91m11.897622108459473: \u001b[0m\u001b[93m understanding the intuition with hands - on pytorch code\u001b[0m\n",
            "\u001b[91m6.980084419250488: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m5.007373332977295: \u001b[0m\u001b[93m natural language understanding\u001b[0m\n",
            "\u001b[91m3.605898857116699: \u001b[0m\u001b[93m fine tune bert ? [SEP] thank you for reading more from me .\u001b[0m\n",
            "\u001b[91m5.693564414978027: \u001b[0m\u001b[93m representing text in natural language processing\u001b[0m\n",
            "\u001b[91m7.268257141113281: \u001b[0m\u001b[93m gentle review\u001b[0m\n",
            "\u001b[91m6.980084419250488: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m8.204219818115234: \u001b[0m\u001b[93m step by step tutorial\u001b[0m\n",
            "\u001b[91m8.895769119262695: \u001b[0m\u001b[93m the ultimate beginner guide for understanding , building and training gans with bulletproof python code\u001b[0m\n",
            "\u001b[91m6.980084419250488: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m6.3194684982299805: \u001b[0m\u001b[93m the ultimate beginner guide to tensorflow\u001b[0m\n",
            "\u001b[91m6.785809516906738: \u001b[0m\u001b[93m how to implement linear regression and gradient descent from scratch\u001b[0m\n",
            "\u001b[91m6.980084419250488: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m4.301733016967773: \u001b[0m\u001b[93m measure\u001b[0m\n",
            "\u001b[91m6.348319053649902: \u001b[0m\u001b[93m a hands - on tutorial on bayesian estimation of epistemic and aleatoric uncertainty with keras\u001b[0m\n",
            "\u001b[91m6.980084419250488: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m5.294600486755371: \u001b[0m\u001b[93m certified learner\u001b[0m\n",
            "\u001b[91m4.9740424156188965: \u001b[0m\u001b[93m follow\u001b[0m\n",
            "\u001b[91m6.097539901733398: \u001b[0m\u001b[93m daily pick\u001b[0m\n",
            "\u001b[91m5.028944492340088: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m9.269716262817383: \u001b[0m\u001b[93m hands - on real - world examples , research , tutorials , and cutting - edge techniques delivered monday to thursday\u001b[0m\n",
            "\u001b[91m6.306863307952881: \u001b[0m\u001b[93m your email\u001b[0m\n",
            "\u001b[91m7.718658447265625: \u001b[0m\u001b[93m get this newsletter\u001b[0m\n",
            "\u001b[91m9.36648178100586: \u001b[0m\u001b[93m review our privacy policy\u001b[0m\n",
            "\u001b[91m8.126276016235352: \u001b[0m\u001b[93m machine learning\u001b[0m\n",
            "\u001b[91m5.584634780883789: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m6.549056053161621: \u001b[0m\u001b[93m artificial intelligence\u001b[0m\n",
            "\u001b[91m4.815519332885742: \u001b[0m\u001b[93m data science\u001b[0m\n",
            "\u001b[91m4.9740424156188965: \u001b[0m\u001b[93m follow\u001b[0m\n",
            "\u001b[91m4.496644020080566: \u001b[0m\u001b[93m a medium publication\u001b[0m\n",
            "\u001b[91m5.586538791656494: \u001b[0m\u001b[93m robert ( munro ) monarch\u001b[0m\n",
            "\u001b[91m5.720642566680908: \u001b[0m\u001b[93m diversity sampling\u001b[0m\n",
            "\u001b[91m7.667279243469238: \u001b[0m\u001b[93m diversity sampling\u001b[0m\n",
            "\u001b[91m5.005545616149902: \u001b[0m\u001b[93m diversity sampling\u001b[0m\n",
            "\u001b[91m8.135485649108887: \u001b[0m\u001b[93m sampling for low activation in your logits and hidden layers\u001b[0m\n",
            "\u001b[91m6.673503875732422: \u001b[0m\u001b[93m cluster - based sampling\u001b[0m\n",
            "\u001b[91m4.671764373779297: \u001b[0m\u001b[93m read\u001b[0m\n",
            "\u001b[91m6.688007831573486: \u001b[0m\u001b[93m indexing and slicing python lists\u001b[0m\n",
            "\u001b[91m5.005056381225586: \u001b[0m\u001b[93m a guide for beginners\u001b[0m\n",
            "\u001b[91m6.678541660308838: \u001b[0m\u001b[93m image for post\u001b[0m\n",
            "\u001b[91m6.971120834350586: \u001b[0m\u001b[93m indexing and slicing lists\u001b[0m\n",
            "\u001b[91m6.523470878601074: \u001b[0m\u001b[93m being familiar with the ins and outs\u001b[0m\n",
            "\u001b[91m5.5889692306518555: \u001b[0m\u001b[93m definitions and stage - setting\u001b[0m\n",
            "\u001b[91m5.684450149536133: \u001b[0m\u001b[93m indexing\u001b[0m\n",
            "-------------------------- Question -----------------------------------------------------\n",
            ":: how to fine tune bert?\n",
            "------------------------- Answer  ------------------------------------------------------\n",
            "\u001b[91m9.955744743347168: \u001b[0m\u001b[93m as we feed input data\u001b[0m\n",
            "As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. Training the classifier is relatively inexpensive. The bottom layers have already great English words representation, and we only really need to train the top layer, with a bit of tweaking going on in the lower levels to accommodate our task. This is a variant of transfer learning.\n",
            "\u001b[92m10.128400802612305: \u001b[0m\u001b[93m taking a sequence of words as input which keep flowing up the stack from one encoder to the next , while new sequences are coming in\u001b[0m\n",
            "BERT works similarly to the Transformer encoder stack, by taking a sequence of words as input which keep flowing up the stack from one encoder to the next, while new sequences are coming in. The final output for each sequence is a vector of 728 numbers in Base or 1024 in Large version. We will use such vectors for our intent classification problem.\n",
            "\u001b[92m10.687943458557129: \u001b[0m\u001b[93m hugging face provides pytorch - transformers repository with additional libraries for interfacing more pre - trained models for natural language processing\u001b[0m\n",
            "We will use the PyTorch interface for BERT by Hugging Face, which at the moment, is the most widely accepted and most powerful PyTorch interface for getting on rails with BERT. Hugging Face provides pytorch-transformers repository with additional libraries for interfacing more pre-trained models for natural language processing: GPT, GPT-2, Transformer-XL, XLNet, XLM.\n",
            "\u001b[92m10.690736770629883: \u001b[0m\u001b[93m on your own dataset\u001b[0m\n",
            "In this article, I demonstrated how to load the pre-trained BERT model in a PyTorch notebook and fine-tune it\u001b[93m on your own dataset\u001b[0m \n",
            "\u001b[92m11.107457160949707: \u001b[0m\u001b[93m minimal task - specific fine - tuning\u001b[0m\n",
            "BERT was released to the public, as a new era in NLP. Its open-sourced model code broke several records for difficult language-based tasks. The pre-trained model on massive datasets enables anyone building natural language processing to use this free powerhouse. BERT theoretically allows us to smash multiple benchmarks with minimal task-specific fine-tuning.\n",
            "\u001b[92m11.362199783325195: \u001b[0m\u001b[93m pre - trained bert model it is possible to quickly and effectively create a high - quality model with minimal effort and training time using the pytorch interface\u001b[0m\n",
            "With BERT we are able to get a good score (95.93%) on the intent classification task. This demonstrates that with a pre-trained BERT model it is possible to quickly and effectively create a high-quality model with minimal effort and training time using the PyTorch interface.\n",
            "\u001b[92m11.36329460144043: \u001b[0m\u001b[93m fully bidirectional\u001b[0m\n",
            "Proper language representation is key for general-purpose language understanding by machines. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word �bank� would have the same representation in �bank deposit� and in �riverbank�. Contextual models instead generate a representation of each word that is based on the other words in the sentence. BERT, as a contextual model, captures these relationships in a bidirectional way. BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is\u001b[93m fully bidirectional\u001b[0m.\n",
            "\u001b[92m11.89180850982666: \u001b[0m\u001b[93m classification ) with own data\u001b[0m\n",
            "We will use BERT to extract high-quality language features from the ATIS query text data, and fine-tune BERT on a specific task (classification) with own data to produce state of the art predictions.\n",
            "\u001b[92m11.897622108459473: \u001b[0m\u001b[93m understanding the intuition with hands - on pytorch code\u001b[0m\n",
            "Understanding the intuition with hands-on PyTorch code for BERT fine-tuned on SQuAD.\n",
            "\u001b[92m11.897622108459473: \u001b[0m\u001b[93m understanding the intuition with hands - on pytorch code\u001b[0m\n",
            "Understanding the intuition with hands-on PyTorch code for BERT fine-tuned on SQuAD.\n",
            "\u001b[92m12.024656295776367: \u001b[0m\u001b[93m on the squad dataset\u001b[0m\n",
            "My new article provides hands-on proven PyTorch code for question answering with BERT fine-tuned\u001b[93m on the SQuAD dataset\u001b[0m.\n",
            "\u001b[92m12.107767105102539: \u001b[0m\u001b[93m pre - training process\u001b[0m\n",
            "BERT�s clever language modeling task masks 15% of words in the input and asks the model to predict the missing word. To make BERT better at handling relationships between multiple sentences, the pre-training process also included an additional task: given two sentences (A and B), is B likely to be the sentence that follows A? Therefore we need to tell BERT what task we are solving by using the concept of attention mask and segment mask. In our case, all words in a query will be predicted and we do not have multiple sentences per query. We define the mask below.\n",
            "\u001b[92m12.368476867675781: \u001b[0m\u001b[93m using our data\u001b[0m\n",
            "Now it is time to create all tensors and iterators needed during fine-tuning of BERT\u001b[93m using our data\u001b[0m.\n",
            "\u001b[92m12.973657608032227: \u001b[0m\u001b[93m bertforsequenceclassification\u001b[0m\n",
            "Finally, it is time to fine-tune the BERT model so that it outputs the intent class given a user query string. For this purpose, we use the\u001b[93m BertForSequenceClassification\u001b[0m,\n",
            "\u001b[92m13.941883087158203: \u001b[0m\u001b[93m python code\u001b[0m\n",
            "In this article, we will demonstrate Transformer, especially how its attention mechanism helps in solving the intent classification task by learning contextual relationships. After demonstrating the limitation of a LSTM-based classifier, we introduce BERT: Pre-training of Deep Bidirectional Transformers, a novel Transformer-approach, pre-trained on large corpora and open-sourced. The last part of this article presents the\u001b[93m Python code\u001b[0m \n",
            "############################### For Question ###############################\n",
            "\u001b[95mhow to fine tune bert?\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}