{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markdown Guide",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9202754471f44bb2a7c92b75c4484e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c933f483f21c4e2cb6d85be10bc3c1b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a09292a768d14582b969cf41aef1c18c",
              "IPY_MODEL_475375c33a884d86b788eca3bba1169d"
            ]
          }
        },
        "c933f483f21c4e2cb6d85be10bc3c1b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a09292a768d14582b969cf41aef1c18c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4daa0dbb01d340229cb59b6816af2dc6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f95ab3adbe642fc88e6afe4295bfe98"
          }
        },
        "475375c33a884d86b788eca3bba1169d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_69c64d14ac0f4d4b8f2d8e2d19192ea9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.36MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff51035b3f644c2a938b2fa20a9ed11c"
          }
        },
        "4daa0dbb01d340229cb59b6816af2dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f95ab3adbe642fc88e6afe4295bfe98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69c64d14ac0f4d4b8f2d8e2d19192ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff51035b3f644c2a938b2fa20a9ed11c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maVdFSBvU1xz"
      },
      "source": [
        "##Install Libraries  - Tranformers, TPU (xla)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaRvdoLEVPH4",
        "outputId": "bc1c9feb-d893-4565-8c8e-2e53914ff2a7"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl\r\n",
        "!pip install Transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cloud-tpu-client==0.10\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Collecting torch-xla==1.7\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl (133.6MB)\n",
            "\u001b[K     |████████████████████████████████| 133.6MB 94kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.7)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (53.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.7\n",
            "Collecting Transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from Transformers) (3.4.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from Transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from Transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from Transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from Transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Transformers) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->Transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->Transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->Transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=cc8b99d27ee4011f3e30509285605c8a1afc4b5af3a8bec4ad962648a77b87bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, Transformers\n",
            "Successfully installed Transformers-4.2.2 sacremoses-0.0.43 tokenizers-0.9.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZ-pveTV0UT"
      },
      "source": [
        "##Import libaries and set up TPU\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9202754471f44bb2a7c92b75c4484e95",
            "c933f483f21c4e2cb6d85be10bc3c1b2",
            "a09292a768d14582b969cf41aef1c18c",
            "475375c33a884d86b788eca3bba1169d",
            "4daa0dbb01d340229cb59b6816af2dc6",
            "8f95ab3adbe642fc88e6afe4295bfe98",
            "69c64d14ac0f4d4b8f2d8e2d19192ea9",
            "ff51035b3f644c2a938b2fa20a9ed11c"
          ]
        },
        "id": "e24Ac1cpXacD",
        "outputId": "ea379602-5bcb-445f-9769-2b9155383dc4"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "from transformers import BertForQuestionAnswering\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "# imports the torch_xla package\r\n",
        "import torch_xla\r\n",
        "import torch_xla.core.xla_model as xm\r\n",
        "\r\n",
        "# Creates a random tensor on xla:1 (a Cloud TPU core)\r\n",
        "dev = xm.xla_device()\r\n",
        "\r\n",
        "#Model\r\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\r\n",
        "model = model.to(dev)\r\n",
        "#Tokenizer\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\r\n",
        "\r\n",
        "#Read test file from Github\r\n",
        "\r\n",
        "import requests\r\n",
        "url = \"https://raw.githubusercontent.com/newspap/MyRepo/main/BERT_for_dummies.txt\"\r\n",
        "resp = requests.get(url)\r\n",
        "input_ = resp.text.split(\"\\r\\n\") \r\n",
        "input_ = list(filter(lambda x: len(x) > 5, input_))\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9202754471f44bb2a7c92b75c4484e95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<Response [200]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbD3KkFQZIOJ",
        "outputId": "01093cc8-bdaa-431f-aa5f-a36d4b85f394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import sample \r\n",
        "input_ = resp.text.split(\"\\r\\n\") \r\n",
        "input_ = list(filter(lambda x: len(x) > 5, input_))\r\n",
        "print(sample(input_, 10))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Uncertainty in Deep Learning. How To Measure?', 'Diversity Sampling Cheatsheet', 'Since we were not quite successful at augmenting the dataset, now, we will rather reduce the scope of the problem. We define a binary classification task where the �flight� queries are evaluated against the remaining classes, by collapsing them into a single class called �other�. The distribution of labels in this new dataset is given below.', 'Image for post', 'towardsdatascience.com', 'Hands-on real-world examples, research, tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look', 'Sign up for The Daily Pick', 'My new article provides hands-on proven PyTorch code for question answering with BERT fine-tuned on the SQuAD dataset.', 'Fellow Top Medium Writer. Fellow of Harvard University. Certified Learner. I help curious minds become AI practitioner.', 'The ultimate beginner guide for understanding, building and training GANs with bulletproof Python code.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GWU2AibZ-Sa"
      },
      "source": [
        "##Continue ask questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxhTyK7KaDjT",
        "outputId": "4cb038d3-7f46-4eb2-f395-71a2e32b4cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class bcolors:\r\n",
        "    HEADER = '\\033[95m'\r\n",
        "    OKBLUE = '\\033[94m'\r\n",
        "    OKCYAN = '\\033[96m'\r\n",
        "    OKGREEN = '\\033[92m'\r\n",
        "    WARNING = '\\033[93m'\r\n",
        "    FAIL = '\\033[91m'\r\n",
        "    ENDC = '\\033[0m'\r\n",
        "    BOLD = '\\033[1m'\r\n",
        "    UNDERLINE = '\\033[4m'\r\n",
        "\r\n",
        "def group_words(s, n):\r\n",
        "    words = s.split()\r\n",
        "    for i in range(0, len(words), n):\r\n",
        "        yield ' '.join(words[i:i+n])\r\n",
        "\r\n",
        "def get_loss(ans):\r\n",
        "    return ans.get('loss')\r\n",
        "\r\n",
        "while True:\r\n",
        "    # if doc changed, reload it\r\n",
        "\r\n",
        "    answers=[]\r\n",
        "    question = input(bcolors.OKBLUE + \"Question: \" + bcolors.ENDC)\r\n",
        "\r\n",
        "    if question == '' :\r\n",
        "     \r\n",
        "        break\r\n",
        "    else:\r\n",
        "        for longParagraph in input_:\r\n",
        "            for paragraph in list(group_words(longParagraph, 180)):\r\n",
        "                #encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)\r\n",
        "\r\n",
        "                #inputs = encoding['input_ids']  #Token embeddings\r\n",
        "                #sentence_embedding = encoding['token_type_ids']  #Segment embeddings\r\n",
        "              \r\n",
        "                inputs = tokenizer(question, paragraph, return_tensors='pt')\r\n",
        "                tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].tolist()[0]) #input tokens\r\n",
        "                start_positions = torch.tensor([1])\r\n",
        "                end_positions = torch.tensor([3])\r\n",
        "\r\n",
        "                #outputs = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\r\n",
        "                with torch.no_grad():\r\n",
        "                    inputs = inputs.to(dev)\r\n",
        "                    start_positions = start_positions.to(dev)\r\n",
        "                    end_positions = end_positions.to(dev)\r\n",
        "                    outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\r\n",
        "                loss = outputs.loss\r\n",
        "                #start_scores, end_scores\r\n",
        "                start_index = torch.argmax(outputs.start_logits)\r\n",
        "\r\n",
        "                end_index = torch.argmax(outputs.end_logits)\r\n",
        "\r\n",
        "                answer = ' '.join(tokens[start_index:end_index+1])\r\n",
        "\r\n",
        "                corrected_answer = ''\r\n",
        "\r\n",
        "                for word in answer.split():\r\n",
        "                    \r\n",
        "                    #If it's a subword token\r\n",
        "                    if word[0:2] == '##':\r\n",
        "                        corrected_answer += word[2:]\r\n",
        "                    else:\r\n",
        "                        corrected_answer += ' ' + word\r\n",
        "                #if (corrected_answer.startswith(\" [\")):\r\n",
        "                #    print(corrected_answer)\r\n",
        "                lossVal = loss.item()\r\n",
        "                \r\n",
        "                if (len(corrected_answer) > 0):\r\n",
        "                    print(bcolors.FAIL + str(lossVal) + \": \" + bcolors.ENDC + bcolors.WARNING + corrected_answer + bcolors.ENDC) \r\n",
        "                    #print(paragraph)\r\n",
        "                answers.append({\"loss\":lossVal, \"answer\":corrected_answer, \"paragraph\": paragraph})\r\n",
        "    answers.sort(key=get_loss)\r\n",
        "    print(\"-------------------------- Question -----------------------------------------------------\")\r\n",
        "    print(\":: \" + question)\r\n",
        "    print(\"------------------------- Answer  ------------------------------------------------------\")\r\n",
        "    \r\n",
        "    for ans in answers[-15:]:\r\n",
        "        coloredAns = (bcolors.OKGREEN if ans['loss'] > 10 else bcolors.FAIL) + str(ans['loss']) + \": \" + bcolors.ENDC + bcolors.WARNING + ans['answer'] + bcolors.ENDC\r\n",
        "\r\n",
        "        print(coloredAns)\r\n",
        "        startIdx = ans['paragraph'].lower().find(ans['answer'])\r\n",
        "\r\n",
        "        ansLength = len(ans['answer'])\r\n",
        "\r\n",
        "        if (startIdx != -1):\r\n",
        "            hightlightedPara = ans['paragraph'][0:startIdx] + bcolors.WARNING + \\\r\n",
        "                    ans['paragraph'][startIdx:startIdx + ansLength] + bcolors.ENDC\r\n",
        "            if (startIdx + ansLength < len(ans['paragraph'])):\r\n",
        "                hightlightedPara += ans['paragraph'][startIdx + ansLength]\r\n",
        "        else :\r\n",
        "            hightlightedPara = ans['paragraph']\r\n",
        "        print(hightlightedPara)\r\n",
        "\r\n",
        "    print(\"############################### For Question ###############################\")\r\n",
        "    print(bcolors.HEADER +  question  + bcolors.ENDC)\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mQuestion: \u001b[0mwhat is the topic\n",
            "\u001b[91m9.119222640991211: \u001b[0m\u001b[93m motivation\u001b[0m\n",
            "\u001b[91m7.770511627197266: \u001b[0m\u001b[93m intent classification\u001b[0m\n",
            "\u001b[91m8.401050567626953: \u001b[0m\u001b[93m question answering\u001b[0m\n",
            "\u001b[91m8.623769760131836: \u001b[0m\u001b[93m understanding the intuition with hands - on pytorch code\u001b[0m\n",
            "\u001b[91m5.185699462890625: \u001b[0m\u001b[93m towardsdatascience . com\u001b[0m\n",
            "\u001b[91m8.531424522399902: \u001b[0m\u001b[93m ambiguous intent labeling\u001b[0m\n",
            "\u001b[91m7.712638854980469: \u001b[0m\u001b[93m transformer , especially how its attention mechanism helps in solving the intent classification task by learning contextual relationships\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}